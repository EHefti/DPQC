{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b681791",
   "metadata": {},
   "source": [
    "## 0. Packages and Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1c5204-8fd1-4af1-8099-cdb44b14371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe926de3-d9ee-463f-9388-a9fffe288d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, display, clear_output\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import sklearn\n",
    "import h5py\n",
    "import spikeinterface.full as si\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.curation as sc\n",
    "import spikeinterface.widgets as sw\n",
    "\n",
    "\n",
    "\n",
    "import bombcell as bc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "sys.path.append(\"/home/ehefti/Github/DPQC\")\n",
    "# sys.path.append(\"C:/Users/elias/OneDrive - ETH Zurich/2025FS - Master Thesis/1 - Scripts/Github/DPQC\")\n",
    "import MaxTwo_Spikesorting.scripts.spike_sorting as ss\n",
    "import MaxTwo_Activity_Screening.screen_maxtwo_activity as sma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e897eb87",
   "metadata": {},
   "source": [
    "## 1. Recording Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1238d33-72ab-4452-a2a1-509e5853d635",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_path = '/net/bs-filesvr02/export/group/hierlemann/recordings/Maxtwo/phornauer/241218/EI_iNeurons/T002523/Network/000020/data.raw.h5'\n",
    "#rec_path = \"D:/Master Thesis/Data/EI_iNeurons/241218/T002523/Network/data.raw.h5\"\n",
    "df_summary, rate_dist, amp_dist = sma.screen_maxtwo_activity(rec_path, segment_duration_s=10,\n",
    "                                                             rate_lower_threshold = 0.5,\n",
    "                                                             rate_upper_threshold = 20,\n",
    "                                                             amp_lower_threshold = 30,\n",
    "                                                             amp_upper_threshold = 80)\n",
    "\n",
    "\n",
    "\n",
    "#log scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f84ba9",
   "metadata": {},
   "source": [
    "## 2. Spikesorting\n",
    "_This part of the pipeline is computationally heavy. It is advisable to run this on a GPU or Cluster._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8e006b-1a9f-4c5e-83b3-040f86f37f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose sorter and set parameters\n",
    "sorter = 'kilosort2_5'\n",
    "si.Kilosort2_5Sorter.set_kilosort2_5_path('/home/ehefti/Github/Kilosort')\n",
    "sorter_params = si.get_default_sorter_params(si.Kilosort2_5Sorter)\n",
    "\n",
    "sorter_params['n_jobs'] = -1\n",
    "sorter_params['detect_threshold'] = 5.5 #6 als Standardwert\n",
    "sorter_params['minFR'] = 0.01 #Lower value -> less units that get automatically deleted\n",
    "sorter_params['minfr_goodchannels'] = 0.01\n",
    "sorter_params['keep_good_only'] = False\n",
    "sorter_params['do_correction'] = False\n",
    "sorter_params['NT'] = 64*1024 + 64 #Batch size -> Wieviel wird auf einmal angeschaut\n",
    "\n",
    "\n",
    "# Set your paths\n",
    "# Linux: '/net/bs-filesvr02/export/group/hierlemann/recordings/Maxtwo/.../data.raw.h5' \n",
    "# Windows: 'S:/group/hierlemann02/recordings/Maxtwo/.../data.raw.h5'\n",
    "\n",
    "rec_path = '/net/bs-filesvr02/export/group/hierlemann/recordings/Maxtwo/phornauer/241218/EI_iNeurons/T002523/Network/000020/data.raw.h5'\n",
    "save_root = '/net/bs-filesvr02/export/group/hierlemann/intermediate_data/Maxtwo/phornauer/EI_iNeurons/241218/T002523/Network/'\n",
    "\"\"\"\n",
    "rec_path = \"D:/Master Thesis/Data/EI_iNeurons/241218/T002523/Network/data.raw.h5\"\n",
    "save_root = \"D:/Master Thesis/Data/EI_iNeurons/241218/T002523/Network/\"\n",
    "\"\"\"\n",
    "h5 = h5py.File(rec_path)\n",
    "stream_ids = list(h5['wells'].keys())\n",
    "stream_ids = stream_ids[0:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a9743-8a70-4b77-86b2-4dbdc48c01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stream_id in tqdm(stream_ids):\n",
    "    h5 = h5py.File(rec_path)\n",
    "    rec_name = list(h5['wells'][stream_id].keys())[0]\n",
    "    rec = si.MaxwellRecordingExtractor(rec_path, stream_id=stream_id, rec_name=rec_name)\n",
    "    ss.clean_sorting(rec, save_root, stream_id=stream_id, sorter=sorter, sorter_params=sorter_params, clear_files=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a3d9c7",
   "metadata": {},
   "source": [
    "## 3. Qualitycontrol (Bombcell & SI-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5098bf7",
   "metadata": {},
   "source": [
    "### 3.1 Bombcell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc37975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your kilosort directory\n",
    "well_id = 'well010'\n",
    "ks_dir = 'D:/Master Thesis/Data/EI_iNeurons/241218/T002523/Network/well000/sorter_output/' #Path(save_root) / well_id / 'sorter_output'\n",
    "\n",
    "# Set bombcell's output directory\n",
    "save_path = Path(ks_dir) / \"bc_output_test\"\n",
    "if not save_path.exists():\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Using kilosort directory: {ks_dir}\")\n",
    "\n",
    "param = bc.get_default_parameters(ks_dir, kilosort_version=2)\n",
    "\n",
    "# Modify some parameters\n",
    "param['ephys_sample_rate'] = 10000  # Sample rate in Hz\n",
    "\n",
    "h5 = h5py.File(rec_path)\n",
    "rec_name = list(h5['wells'][well_id].keys())[0]\n",
    "rec = si.MaxwellRecordingExtractor(rec_path, stream_id=well_id, rec_name=rec_name)\n",
    "\n",
    "param['nChannels'] = rec.get_num_channels()\n",
    "param['raw_data_file'] = Path(ks_dir) / \"recording.dat\"\n",
    "param['ephysKilosortPath']\n",
    "\n",
    "pprint(param)\n",
    "\n",
    "(quality_metrics, param, unit_type, unit_type_string) = bc.run_bombcell(ks_dir, save_path, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a006b2",
   "metadata": {},
   "source": [
    "See the metrics and the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236cde51",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_metrics_table = pd.DataFrame(quality_metrics)\n",
    "quality_metrics_table.insert(0, 'Bombcell_unit_type', unit_type_string)\n",
    "quality_metrics_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fed00",
   "metadata": {},
   "source": [
    "Take a look at your units in a GUI. Potentially adjust thresholds or change the classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8bd439",
   "metadata": {},
   "outputs": [],
   "source": [
    "gui = bc.unit_quality_gui(\n",
    "    ks_dir=ks_dir,\n",
    "    quality_metrics=quality_metrics,\n",
    "    unit_types=unit_type,\n",
    "    param=param,\n",
    "    save_path=save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf5dfc",
   "metadata": {},
   "source": [
    "#### Bombcell Philipp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387589f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxw_bombcell.generate_sorting_path_list import generate_sorting_path_list\n",
    "from mxw_bombcell.infer_sampling_rate import infer_sampling_rate\n",
    "from mxw_bombcell.bombcell_to_phy import bombcell_to_phy\n",
    "\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "qc_root_path = 'D:/Master Thesis/Data/EI_iNeurons'\n",
    "path_logic = ['24*', 'T002523', 'Network', 'well*', 'sorter_output']\n",
    "\n",
    "sorting_path_list = generate_sorting_path_list(qc_root_path, path_logic)\n",
    "print(f\"Found {len(sorting_path_list)} sortings\")\n",
    "\n",
    "# PARAMS\n",
    "rerun = 1  # 1 to re-run, 0 to skip if metrics are present\n",
    "gain_to_uV = 6.2\n",
    "verbose = 0\n",
    "\n",
    "for i, sorting_path in enumerate(sorting_path_list):\n",
    "    ephysKilosortPath = sorting_path\n",
    "    sampling_rate = infer_sampling_rate(ephysKilosortPath)\n",
    "    \n",
    "    kilosortVersion = 2 # Change if using Kilosort4\n",
    "    savePath = os.path.join(ephysKilosortPath, \"bc_output\")\n",
    "\n",
    "    param = bc.get_default_parameters(kilosort_path=ephysKilosortPath, kilosort_version=kilosortVersion, gain_to_uV=gain_to_uV)\n",
    "\n",
    "    param['extractRaw'] = 0\n",
    "    param['ephys_sample_rate'] = sampling_rate\n",
    "\n",
    "    param['nSyncChannels'] = 0\n",
    "    param['removeDuplicateSpikes'] = 0\n",
    "    param['minWvDuration'] = 200\n",
    "    param['maxWvDuration'] = 1500\n",
    "    param['tauR_valuesMin'] = 0.001\n",
    "    param['tauR_valuesMax'] = 0.003\n",
    "    param['maxRPVviolations'] = 0.05\n",
    "    param['verbose'] = verbose\n",
    "    param['computeDistanceMetrics'] = 1\n",
    "\n",
    "    qMetricsExist = (len(glob(os.path.join(savePath, 'qMetric*.mat'))) > 0) or \\\n",
    "                    (os.path.exists(os.path.join(savePath, 'templates._bc_qMetrics.parquet')))\n",
    "\n",
    "    if not qMetricsExist or rerun:\n",
    "        spikeTimes_samples, spikeTemplates, templateWaveforms, templateAmplitudes, pcFeatures, \\\n",
    "        pcFeatureIdx, channelPositions = bc.load_ephys_data(ephysKilosortPath)\n",
    "        \n",
    "        if len(spikeTimes_samples) > 0 and param['ephys_sample_rate'] > 0:\n",
    "            param['minNumSpikes'] = round(np.max(spikeTimes_samples) / param['ephys_sample_rate'] / 20)\n",
    "        else:\n",
    "            param['minNumSpikes'] = 0\n",
    "\n",
    "        param['nChannels'] = channelPositions.shape[0] if channelPositions is not None else 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        (\n",
    "            qMetrics,\n",
    "            param,\n",
    "            unitType,\n",
    "            unitTypeString,\n",
    "        ) = bc.run_bombcell(\n",
    "            ephysKilosortPath, savePath, param\n",
    "        )\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        qMetric, runtimes = bc.get_all_quality_metrics(param=param, \n",
    "                                                       unique_templates=None,\n",
    "                                                       spike_times_seconds=spikeTimes_samples, \n",
    "                                                       spike_clusters=spikeTemplates, \n",
    "                                                       template_waveforms=templateWaveforms, \n",
    "                                                       template_amplitudes=templateAmplitudes,\n",
    "                                                       time_chunks=None,\n",
    "                                                       pc_features=pcFeatures, \n",
    "                                                       pc_features_idx=pcFeatureIdx,\n",
    "                                                       channel_positions=channelPositions, \n",
    "                                                       save_path=savePath)\n",
    "        \n",
    "        unitType, unitTypeString = bc.get_quality_unit_type(param, qMetric)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "bc_path_list = [os.path.join(sp, \"bc_output\") for sp in sorting_path_list]\n",
    "\n",
    "overwrite = True\n",
    "bombcell_to_phy(bc_path_list[:3], overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6451ac12",
   "metadata": {},
   "source": [
    "### 3.2 SI-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca51a643",
   "metadata": {},
   "source": [
    "#### Train Model\n",
    "You can train your own Model if you want - this is not necessary though. There is a model that works reasonably well for MaxTwo EPhys data. If you train your own model it might be more precise for your cell-line, but it takes a while to label your units manually and train the model to get reasonable results. The pretrained Model will be loaded in the next step of the pipeline, you can jump to \"Apply Model\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2e4df7",
   "metadata": {},
   "source": [
    "##### Manual Labeling\n",
    "In order to label your units manually and train your model, download phy and follow the instructions on the following GitHub repository: `https://github.com/cortex-lab/phy/`\n",
    "\n",
    "The documentation to explain the GUI can be found here `https://phy.readthedocs.io/en/latest/`\n",
    "\n",
    "You can also start the GUI from python, but you will have to change the kernel which makes you loose your cached variables. Use the code below:\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from phy.apps.template import template_gui\n",
    "from pathlib import Path\n",
    "\n",
    "save_root = 'your/path/to/your/sorted/wells/'\n",
    "well_id = 'well010'\n",
    "params_path = Path(save_root) / well_id / 'sorter_output' / 'params.py'\n",
    "template_gui(params_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade41bc5",
   "metadata": {},
   "source": [
    "##### Training your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec57870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, you can set the number of cores you use using e.g.\n",
    "# si.set_global_job_kwargs(n_jobs = 8)\n",
    "\n",
    "os.environ['HDF5_PLUGIN_PATH'] = 'C:/Users/elias/OneDrive - ETH Zurich/2025FS - Master Thesis/1 - Scripts/Github/DPQC/MaxTwo_Quality_Control/'\n",
    "\n",
    "# Choose sorting to train the model on\n",
    "well_id = 'well001'\n",
    "path_to_sorting = Path(save_root) / well_id / 'sorter_output'\n",
    "sorting_train = si.read_kilosort(folder_path=path_to_sorting)\n",
    "\n",
    "# Real Recording Loading\n",
    "h5 = h5py.File(rec_path)\n",
    "rec_name = list(h5['wells'][well_id].keys())[0]\n",
    "rec_train = si.MaxwellRecordingExtractor(rec_path, stream_id=well_id, rec_name=rec_name)\n",
    "\"\"\"\n",
    "# For testing purposes\n",
    "len_1 = 15\n",
    "len_2 = 15\n",
    "rec_train, sorting_1 = si.generate_ground_truth_recording(num_channels=4, seed=1, num_units=len_1)\n",
    "_, sorting_2 =si.generate_ground_truth_recording(num_channels=4, seed=2, num_units=len_2)\n",
    "\n",
    "sorting_train = si.aggregate_units([sorting_1, sorting_2])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "analyzer = si.create_sorting_analyzer(sorting=sorting_train, recording=rec_train)\n",
    "analyzer.compute(['noise_levels','random_spikes','waveforms','templates'])\n",
    "analyzer.compute(['spike_locations','spike_amplitudes','correlograms','principal_components','quality_metrics','template_metrics'])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "label_path = Path(save_root) / well_id / 'sorter_output' / 'cluster_bc_unitType.tsv'\n",
    "df = pd.read_csv(label_path, sep='\\t')\n",
    "manual_labels = ['good' if unit_type == 'GOOD' else 'bad' for unit_type in df['bc_unitType']]\n",
    "\"\"\"\n",
    "\n",
    "#labels = pd.read_csv(path_to_sorting / 'cluster_KSLabel.tsv', sep='\\t')\n",
    "# manual_labels = ['good', 'good', 'good', 'good', 'good', 'bad', 'bad', 'bad', 'bad', 'bad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.plot_unit_templates(analyzer, unit_ids=[0, 1], scale=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babdbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spikeinterface.extractors.toy_example import generate_ground_truth_recording\n",
    "\n",
    "# --- Define Output Directory for Labels ---\n",
    "# IMPORTANT: Change this to your desired output directory, e.g., your sorter_output folder.\n",
    "# This path should exist or be created before exporting.\n",
    "# output_dir_for_labels = path_to_sorting # Change if needed\n",
    "\n",
    "output_dir_for_labels = 'KiloSort_Quality_Control/'\n",
    "\n",
    "\n",
    "# --- Load Your SpikeInterface Data ---\n",
    "# Ensure rec_train, sorting_train, and analyzer are defined BEFORE this cell runs.\n",
    "# If not, toy data will be generated for demonstration purposes.\n",
    "if 'rec_train' not in locals() or 'sorting_train' not in locals() or 'analyzer' not in locals():\n",
    "    print(\"WARNING: rec_train, sorting_train, or analyzer not defined. Generating toy data.\")\n",
    "    \n",
    "    recording = generate_ground_truth_recording(\n",
    "        durations=[10], sampling_frequency=30000, num_channels=4, seed=42\n",
    "    )\n",
    "    from spikeinterface.sorting_tools import NumpySorting\n",
    "    spike_times = np.array([1000, 1050, 2000, 2050, 3000, 3050, 4000, 4050])\n",
    "    spike_clusters = np.array([0, 0, 1, 1, 2, 2, 3, 3])\n",
    "    sorting = NumpySorting.from_times_labels(spike_times, spike_clusters, sampling_frequency=recording.sampling_frequency)\n",
    "    \n",
    "    import tempfile\n",
    "    temp_waveforms_folder = Path(tempfile.mkdtemp())\n",
    "    analyzer = si.create_sorting_analyzer(sorting, recording, folder=temp_waveforms_folder, format=\"binary\", sparse=False)\n",
    "    analyzer.compute(\"random_spikes\")\n",
    "    analyzer.compute(\"unit_waveforms\")\n",
    "    analyzer.compute(\"correlograms\")\n",
    "    print(f\"Loaded {len(sorting.get_unit_ids())} units from toy data.\")\n",
    "else:\n",
    "    recording = rec_train\n",
    "    sorting = sorting_train\n",
    "    print(f\"Using pre-defined rec_train, sorting_train, and analyzer. Loaded {len(sorting.get_unit_ids())} units.\")\n",
    "\n",
    "# --- Initialize Unit Properties for Labeling ---\n",
    "unit_ids = sorting.get_unit_ids()\n",
    "initial_labels = ['unlabeled'] * len(unit_ids)\n",
    "sorting.set_property('quality_label', initial_labels)\n",
    "\n",
    "current_unit_labels = {unit_id: 'unlabeled' for unit_id in unit_ids}\n",
    "for i, unit_id in enumerate(unit_ids):\n",
    "    current_unit_labels[unit_id] = sorting.get_property('quality_label')[i]\n",
    "\n",
    "# --- Create Interactive Widgets ---\n",
    "unit_selector = widgets.Dropdown(\n",
    "    options=unit_ids,\n",
    "    value=unit_ids[0],\n",
    "    description='Select Unit:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "output_plot = widgets.Output()\n",
    "\n",
    "button_good = widgets.Button(description=\"Label as Good\", button_style='success')\n",
    "button_mua = widgets.Button(description=\"Label as MUA\", button_style='warning')\n",
    "button_noise = widgets.Button(description=\"Label as Noise\", button_style='danger')\n",
    "button_unlabeled = widgets.Button(description=\"Unlabel\", button_style='info')\n",
    "button_export = widgets.Button(description=\"Export Labels to TSV\", button_style='primary') # New button\n",
    "\n",
    "label_status = widgets.Textarea(\n",
    "    value='Labels will appear here.',\n",
    "    description='Unit Labels:',\n",
    "    disabled=True,\n",
    "    layout=widgets.Layout(width='auto', height='150px')\n",
    ")\n",
    "\n",
    "# --- Define Update and Labeling Functions ---\n",
    "def update_plot(change):\n",
    "    selected_unit_id = change['new']\n",
    "    with output_plot:\n",
    "        clear_output(wait=True)\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "        sw.plot_unit_templates(analyzer, unit_ids=[selected_unit_id], ax=axes[0], same_axis=True, scale=5)\n",
    "        axes[0].set_title(f\"Waveforms for Unit {selected_unit_id} (Label: {current_unit_labels[selected_unit_id]})\")\n",
    "\n",
    "        sw.plot_autocorrelograms(analyzer, unit_ids=[selected_unit_id], ax=axes[1])\n",
    "        axes[1].set_title(f\"Autocorrelogram for Unit {selected_unit_id}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def update_label_status():\n",
    "    sorted_labels = current_unit_labels.items()\n",
    "    label_text = \"\\n\".join([f\"Unit {uid}: {label}\" for uid, label in sorted_labels])\n",
    "    label_status.value = label_text\n",
    "\n",
    "def apply_label(b):\n",
    "    selected_unit_id = unit_selector.value\n",
    "    label = b.description.split(\" \")[2].lower()\n",
    "    if label == \"unlabel\":\n",
    "        label = \"unlabeled\"\n",
    "\n",
    "    current_unit_labels[selected_unit_id] = label\n",
    "    idx = np.where(unit_ids == selected_unit_id)[0][0] \n",
    "    new_property_values = list(sorting.get_property('quality_label'))\n",
    "    new_property_values[idx] = label\n",
    "    sorting.set_property('quality_label', new_property_values)\n",
    "\n",
    "    with output_plot:\n",
    "        clear_output(wait=True)\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "        sw.plot_unit_templates(analyzer, unit_ids=[selected_unit_id], ax=axes[0], same_axis=True, scale=5)\n",
    "        axes[0].set_title(f\"Waveforms for Unit {selected_unit_id} (Label: {current_unit_labels[selected_unit_id]})\")\n",
    "\n",
    "        sw.plot_autocorrelograms(analyzer, unit_ids=[selected_unit_id], ax=axes[1])\n",
    "        axes[1].set_title(f\"Autocorrelogram for Unit {selected_unit_id}\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    update_label_status()\n",
    "    print(f\"Unit {selected_unit_id} labeled as '{label}'\")\n",
    "\n",
    "    # --- Go to next unit automatically ---\n",
    "    current_unit_index = list(unit_ids).index(selected_unit_id)\n",
    "    next_unit_index = current_unit_index + 1\n",
    "\n",
    "    if next_unit_index < len(unit_ids):\n",
    "        unit_selector.value = unit_ids[next_unit_index]\n",
    "    else:\n",
    "        print(\"All units reviewed!\")\n",
    "        # Optionally, you could reset to the first unit or disable the selector\n",
    "        # unit_selector.value = unit_ids[0]\n",
    "        # unit_selector.disabled = True\n",
    "\n",
    "def export_labels_to_tsv(b):\n",
    "    \"\"\"Exports the current unit labels to a TSV file.\"\"\"\n",
    "    labels_df = pd.DataFrame({\n",
    "        'unit_id': list(current_unit_labels.keys()),\n",
    "        'quality_label': list(current_unit_labels.values())\n",
    "    })\n",
    "    output_filepath = Path(output_dir_for_labels) / \"manual_unit_labels.tsv\"\n",
    "    labels_df.to_csv(output_filepath, sep='\\t', index=False)\n",
    "    print(f\"Labels exported to: {output_filepath}\")\n",
    "\n",
    "# --- Connect Widgets to Functions ---\n",
    "unit_selector.observe(update_plot, names='value')\n",
    "\n",
    "button_good.on_click(apply_label)\n",
    "button_mua.on_click(apply_label)\n",
    "button_noise.on_click(apply_label)\n",
    "button_unlabeled.on_click(apply_label)\n",
    "button_export.on_click(export_labels_to_tsv) # Connect the new button\n",
    "\n",
    "# Initial plot and label status update\n",
    "update_plot({'new': unit_selector.value})\n",
    "update_label_status()\n",
    "\n",
    "# --- Arrange and Display Widgets ---\n",
    "label_buttons = widgets.HBox([button_good, button_mua, button_noise, button_unlabeled])\n",
    "export_button_box = widgets.HBox([button_export])\n",
    "ui = widgets.VBox([unit_selector, label_buttons, output_plot, label_status, export_button_box])\n",
    "\n",
    "print(\"Displaying interactive GUI...\")\n",
    "display(ui)\n",
    "\n",
    "# Example to access labeled data:\n",
    "# good_units = [uid for uid, label in current_unit_labels.items() if label == 'good']\n",
    "# print(\"\\nExample: Good units after labeling:\", good_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68422457",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_labels = pd.read_csv(Path(output_dir_for_labels) / \"manual_unit_labels.tsv\", sep='\\t')\n",
    "print(manual_labels['quality_label'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6823f0d-2ec1-4855-9ece-b8070926ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"models/\"\n",
    "manual_labels = pd.read_csv(Path(output_dir_for_labels) / \"manual_unit_labels.tsv\", sep='\\t')\n",
    "manual_labels = manual_labels['quality_label'].tolist()\n",
    "\n",
    "trainer = sc.train_model(\n",
    "    mode=\"analyzers\",\n",
    "    labels=[manual_labels],\n",
    "    analyzers=[analyzer],\n",
    "    folder=model_folder,\n",
    "    overwrite=True, # Set to True if you want to overwrite existing models\n",
    "    metric_names = None, # Specify which metrics to use for training: by default uses those already calculted\n",
    "    imputation_strategies = [\"median\"], # Defaults to all\n",
    "    scaling_techniques = [\"standard_scaler\"], # Defaults to all\n",
    "    classifiers = None, # Default to Random Forest only. Other classifiers you can try [ \"AdaBoostClassifier\",\"GradientBoostingClassifier\",\"LogisticRegression\",\"MLPClassifier\"]\n",
    "    search_kwargs = {'cv': 3} # Parameters used during the model hyperparameter search\n",
    ")\n",
    "\n",
    "best_model = trainer.best_pipeline\n",
    "\n",
    "accuracies = pd.read_csv(Path(model_folder) / \"model_accuracies.csv\", index_col = 0)\n",
    "accuracies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4e6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, model_info = sc.load_model(\n",
    "    model_folder=model_folder,\n",
    "    trusted=['numpy.dtype'] # Specify which modules are trusted\n",
    ")\n",
    "\n",
    "labels_and_probababilities = si.auto_label_units(\n",
    "    sorting_analyzer=analyzer,\n",
    "    model_folder=model_folder,\n",
    "    trust_model=True\n",
    ")\n",
    "\n",
    "\"\"\" \n",
    "# We can load a pretrained model from HuggingFace instead of training one\n",
    "model, model_info = sc.load_model(\n",
    "    sorting_analyzer = analyzer,\n",
    "    repo_id = \"SpikeInterface/toy_tetrode_model\",\n",
    "    trusted = ['numpy.dtype']\n",
    ")\n",
    "\n",
    "\n",
    "# The returned labels and probabilities are stored in the sorting analyzer\n",
    "labels = analyzer.sorting.get_property(\"classifier_label\")\n",
    "probabilities = analyzer.sorting.get_property(\"classifier_probability\")\n",
    "\"\"\"\n",
    "\n",
    "print(labels_and_probababilities)\n",
    "\n",
    "sw.plot_unit_templates(analyzer, unit_ids=['3','9'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2b2e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "label_conversion = model_info['label_conversion']\n",
    "predictions = labels_and_probababilities['prediction']\n",
    "\n",
    "conf_matrix = confusion_matrix(manual_labels, predictions)\n",
    "\n",
    "# Calculate balanced accuracy for the confusion matrix\n",
    "balanced_accuracy = balanced_accuracy_score(manual_labels, predictions)\n",
    "\n",
    "plt.imshow(conf_matrix)\n",
    "for (index, value) in np.ndenumerate(conf_matrix):\n",
    "    plt.annotate( str(value), xy=index, color=\"white\", fontsize=\"15\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Manual Label')\n",
    "plt.xticks(ticks = [0, 1], labels = list(label_conversion.values()))\n",
    "plt.yticks(ticks = [0, 1], labels = list(label_conversion.values()))\n",
    "plt.title('Predicted vs Manual Label')\n",
    "plt.suptitle(f\"Balanced Accuracy: {balanced_accuracy}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc45506",
   "metadata": {},
   "source": [
    "#### Apply Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8193099",
   "metadata": {},
   "outputs": [],
   "source": [
    "const = 1\n",
    "\n",
    "\"\"\"\n",
    "model, model_info = sc.load_model(\n",
    "    repo_id = \"SpikeInterface/UnitRefine_noise_neural_classifier\",\n",
    "    trusted = ['numpy.dtype']\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#for stream_id in tqdm(stream_ids):\n",
    "for i in range(10):\n",
    "    \"\"\"\n",
    "    h5 = h5py.File(path)\n",
    "    rec_name = list(h5['wells'][stream_id].keys())[0]\n",
    "    rec = si.MaxwellRecordingExtractor(path, stream_id=stream_id, rec_name=rec_name)\n",
    "\n",
    "    path_to_sorting = Path(save_root) / stream_id / 'sorter_output'\n",
    "    sorting = si.read_kilosort(folder_path=path_to_sorting)\n",
    "    \"\"\"\n",
    "\n",
    "    rec, sorting_1 = si.generate_ground_truth_recording(num_channels=4, seed=const, num_units=5)\n",
    "    _, sorting_2 =si.generate_ground_truth_recording(num_channels=4, seed=const+1, num_units=5)\n",
    "    const += 2\n",
    "\n",
    "    sorting = si.aggregate_units([sorting_1, sorting_2])\n",
    "\n",
    "    analyzer = si.create_sorting_analyzer(sorting=sorting, recording=rec)\n",
    "    analyzer.compute(['noise_levels','random_spikes','waveforms','templates','spike_locations','spike_amplitudes','correlograms','principal_components', 'quality_metrics', 'template_metrics'])\n",
    "\n",
    "    all_metric_names = list(analyzer.get_extension('quality_metrics').get_data().keys()) + list(analyzer.get_extension('template_metrics').get_data().keys())\n",
    "    print(set(model.feature_names_in_).issubset(set(all_metric_names)))\n",
    "\n",
    "    labels_and_probababilities = si.auto_label_units(\n",
    "        sorting_analyzer=analyzer,\n",
    "        model_folder=model_folder,\n",
    "        #repo_id=\"SpikeInterface/UnitRefine_noise_neural_classifier\",\n",
    "        trust_model=True\n",
    "    )\n",
    "\n",
    "    print(labels_and_probababilities)\n",
    "\n",
    "    #labels_and_probababilities = pd.DataFrame(labels_and_probababilities)\n",
    "    #labels_and_probababilities.to_csv(path_to_sorting / 'labels_and_probabilities.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
